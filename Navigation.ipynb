{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Navigation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGKcmKlfSNB52vrtB5xypS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasad-kumkar/Deep-Learning/blob/master/Navigation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv-D7TH33TML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "\n",
        "import random \n",
        "from collections import namedtuple, deque\n",
        "\n",
        "from unityagents import UnityEnvironment\n",
        "\n",
        "\n",
        "LR = 5e-4\n",
        "TAU = 1e-3\n",
        "GAMMA = 0.99\n",
        "UPDATE_EVERY = 4\n",
        "BUFFER_SIZE = int(1e5)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.sigmoid(self.fc3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA673nbr3WPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size \n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.q_local = QNetwork(state_size, action_size, seed)\n",
        "        self.q_target = QNetwork(state_size, action_size, seed)\n",
        "        self.optimizer = optim.Adam(self.q_local.parameters(), lr=LR)\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        self.t_size = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        self.t_size = (self.t_size+1)%UPDATE_EVERY\n",
        "        if t_size==0:\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                e = self.memory.sample()\n",
        "                self.learn(e)\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)        #Get state\n",
        "        self.q_local.eval()                                         #Set Q_local in evaluate mode\n",
        "        #Equivalent to q_local.train(False)\n",
        "        with torch.no_grad():                                       #Get Action values\n",
        "            action_values = self.q_local(state)\n",
        "        self.q_local.train()                                        #Train Q_local\n",
        "\n",
        "        if random.random()>epsilon:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "    \n",
        "    def learn(self, experiences, gamma=GAMMA):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        #TD target\n",
        "        Q_target_next = self.q_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_target = rewards + Q_target_next*gamma*(1-dones)\n",
        "\n",
        "        #Currently predicted Q value\n",
        "        Q_expected = self.q_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.soft_update(self.q_local, self.q_target)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau=TAU):\n",
        "\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data+(1.0-tau)*target_param.data)\n",
        "\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.batch_size = batch_size \n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        e = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        states = torch.from_numpy(np.vstack([i.state for i in e if i is not None]))\n",
        "        actions = torch.from_numpy(np.vstack([i.action for i in e if i is not None]))\n",
        "        rewards = torch.from_numpy(np.vstack([i.reward for i in e if i is not None]))\n",
        "        next_states = torch.from_numpy(np.vstack([i.next_state for i in e if i is not None]))\n",
        "        dones = torch.from_numpy(np.vstack([i.done for i in e if i is not None]))\n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQZF_veW312t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "f41b0b54-989b-423f-eb70-42aeae6db8e8"
      },
      "source": [
        "env = UnityEnvironment(file_name=\"/Banana.x86_64\")\n",
        "\n",
        "brain_name = env.brain_names[0]\n",
        "brain = env.brains[brain_name]\n",
        "\n",
        "env_info = env.reset(train_mode=True)[brain_name]\n",
        "action_size = brain.vector_action_space_size\n",
        "state = env_info.vector_observations[0]\n",
        "state_size = len(state)\n",
        "\n",
        "agent = Agent(state_size, action_size, seed = 1000)\n",
        "\n",
        "def training(n_episodes=100,max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    scores=[]\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    for i_episode in range(n_episodes):\n",
        "        env_info = env.reset(train_mode=True)[brain_name]\n",
        "        state = env_info.vector_observations[0]\n",
        "        score = 0\n",
        "        done = False\n",
        "        for _ in range(max_t):    \n",
        "            action = agent.act(state, eps)\n",
        "            env_info = env.reset(train_mode=True)[brain_name]\n",
        "            next_state = env_info.vector_observations[0]\n",
        "            reward = env_info.rewards[0]\n",
        "            done = env_info.local_done[0]\n",
        "            score+=reward\n",
        "        scores.append(score)\n",
        "        scores_window.append(score)\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "    return scores\n",
        "\n",
        "score = training()\n",
        "'''\n",
        "while True:\n",
        "    action = agent.act(action_size)\n",
        "    env_info = env.step(action)[brain_name]\n",
        "    next_state = env_info.vector_observations[0]\n",
        "    reward = env_info.rewards[0]\n",
        "    done = env_info.local_done[0]\n",
        "    score += reward\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break\n",
        "print(score)\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9e16bfb414c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Banana.x86_64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbrain_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# and the communicator will directly try to connect to an existing unity environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocker_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_graphics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start training by pressing the Play button in the Unity Editor.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mexecutable_launcher\u001b[0;34m(self, file_name, docker_training, no_graphics)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                     self.proc1 = subprocess.Popen(\n\u001b[0;32m--> 195\u001b[0;31m                         [launch_string, '--port', str(self.port)])\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \"\"\"\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1362\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/Banana.x86_64'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dEoA-HW4Wif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd /"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dODTgqSd4g4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7ff8caf-134c-4f21-95d5-d67b19d68abd"
      },
      "source": [
        "cd"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu-nW7344uMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44uAfPD4vgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPQqYwQL4xnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}